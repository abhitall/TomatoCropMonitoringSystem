{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "brain_tumor_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Install the PyDrive wrapper & import libraries.\r\n",
        "# This only needs to be done once per notebook.\r\n",
        "!pip install -U -q PyDrive\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "\r\n",
        "# Authenticate and create the PyDrive client.\r\n",
        "# This only needs to be done once per notebook.\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)\r\n",
        "\r\n",
        "# Download a file based on its file ID.\r\n",
        "download = drive.CreateFile({'id': '16hA5xU-EmCSRAVEkK_xtYQv8FWyvHOl6'})\r\n",
        "download.GetContentFile('TomatoData.zip')"
      ],
      "outputs": [],
      "metadata": {
        "id": "UUhr10WVNNgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "9bd4252c-77c7-4605-bbfd-481bb585e85d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip TomatoData.zip\r\n",
        "!rm TomatoData.zip"
      ],
      "outputs": [],
      "metadata": {
        "id": "kj9HUXhzcZ7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Part 1 - Building the CNN\r\n",
        "\r\n",
        "# Importing the keras libraries and packages\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\r\n",
        "\r\n",
        "# Initialising the CNN or classifier\r\n",
        "classifier = Sequential()\r\n",
        "\r\n",
        "# Step 1 - Convolution, pooling!\r\n",
        "classifier.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\r\n",
        "classifier.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(MaxPooling2D(pool_size=2))\r\n",
        "classifier.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(MaxPooling2D(pool_size=2))\r\n",
        "classifier.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(MaxPooling2D(pool_size=2))\r\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(MaxPooling2D(pool_size=2))\r\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu'))\r\n",
        "classifier.add(MaxPooling2D(pool_size=2))\r\n",
        "classifier.add(BatchNormalization())\r\n",
        "\r\n",
        "# Step 3 - Flattening\r\n",
        "classifier.add(Flatten())\r\n",
        "\r\n",
        "# Step 4 - Full connection\r\n",
        "classifier.add(Dense(units = 4096, activation = 'relu'))\r\n",
        "classifier.add(Dropout(0.5))\r\n",
        "classifier.add(Dense(units = 4096, activation = 'relu'))\r\n",
        "classifier.add(Dropout(0.5))\r\n",
        "classifier.add(Dense(units = 1000, activation = 'relu'))\r\n",
        "classifier.add(Dropout(0.3))\r\n",
        "classifier.add(Dense(units = 3, activation = 'sigmoid'))\r\n",
        "\r\n",
        "# Compiling the CNN or classifier\r\n",
        "classifier.compile(optimizer = Adam(lr=3e-06, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True), loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "# Part 2 - Fitting the CNN to the images\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "def plotHistory(history):\r\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\r\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\r\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\r\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\r\n",
        "    \r\n",
        "    if len(loss_list) == 0:\r\n",
        "        print('Loss is missing in history')\r\n",
        "        return \r\n",
        "    \r\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\r\n",
        "    \r\n",
        "    ## Loss\r\n",
        "    plt.figure(1)\r\n",
        "    for l in loss_list:\r\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\r\n",
        "    for l in val_loss_list:\r\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\r\n",
        "    \r\n",
        "    plt.title('Loss')\r\n",
        "    plt.xlabel('Epochs')\r\n",
        "    plt.ylabel('Loss')\r\n",
        "    plt.legend()\r\n",
        "    \r\n",
        "    ## Accuracy\r\n",
        "    plt.figure(2)\r\n",
        "    for l in acc_list:\r\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\r\n",
        "    for l in val_acc_list:    \r\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\r\n",
        "\r\n",
        "    plt.title('Accuracy')\r\n",
        "    plt.xlabel('Epochs')\r\n",
        "    plt.ylabel('Accuracy')\r\n",
        "    plt.legend()\r\n",
        "    plt.show()    \r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "training_set = train_datagen.flow_from_directory('TomatoData/train', target_size = (224, 224),batch_size = 32,class_mode = 'categorical')\r\n",
        "\r\n",
        "validation_set = test_datagen.flow_from_directory('TomatoData/val', target_size = (224, 224), batch_size = 32, class_mode = 'categorical')\r\n",
        "\r\n",
        "try:\r\n",
        "  history = classifier.fit_generator(training_set, steps_per_epoch = 94, callbacks = [EarlyStopping('val_acc', mode='auto', patience=3)], epochs = 50, validation_data = validation_set, validation_steps = 37)\r\n",
        "  plotHistory(history)\r\n",
        "finally:\r\n",
        "  classifier.save('model.h5')"
      ],
      "outputs": [],
      "metadata": {
        "id": "KI3pAgUkIxyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "f3209cbb-72ba-4f7a-c801-eb57a0e9701d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# upload weights file to google drive.\r\n",
        "upload = drive.CreateFile({'title': 'model.h5'})\r\n",
        "upload.SetContentFile('model.h5')\r\n",
        "upload.Upload()\r\n",
        "print('Uploaded file with ID {}'.format(upload.get('id')))"
      ],
      "outputs": [],
      "metadata": {
        "id": "ybHj87SXApiS"
      }
    }
  ]
}